import numpy as np 
import pandas as pd 
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt 
import seaborn as sns 
import lightgbm as lgb 

data = pd.read_csv("insurance_claims.csv")
data.head()

data.describe()

# Dropping Columns
data.drop('_c39', axis=1, inplace=True)

# Checking missing values
# Function to calculate missing values by column# Funct
def missing_values_table(df):
    # Total missing values
    mis_val = df.isnull().sum()

    # Percentage of missing values
    mis_val_percent = 100 * df.isnull().sum()/len(df)

    # Make a table with the results
    mis_val_table = pd.concat([mis_val, mis_val_percent],axis=1)

    # Rename the columns
    mis_val_table_ren_columns = mis_val_table.rename(
        columns= {0 : 'Missing Values', 1 : '% of Total Values'}
    )

    # Sort the table by percentage of missing descending
    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)

    # Print some summary information
    print("Your selected dataframe has" + str(df.shape[1])+ "columns.\n"
            "There are" + str(mis_val_table_ren_columns.shape[0])+ "Columns that have missing values.")

    # Return the dataframe with missing information
    return mis_val_table_ren_columns

# Missing values statistics
missing_values = missing_values_table(data)
missing_values


# Lets do lable encoding to make more features
le = LabelEncoder()
le_count = 0

# Iterate through the columns
for col in data:
    if data[col].dtype == 'object':
         # If 2 or fewer unique categories
         if len(list(data[col].unique())) <= 2:
            # Train on the training data
            le.fit(data[col])
            # Transform both training and testing data
            data[col] = le.transform(data[col])

            # Keep track of how many columns were label encoded
            le_count += 1

print('%d columns were label encoded.' % le_count)

sns.set(style="white")

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(15,15))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

sns.heatmap(data.corr(), cmap= cmap, vmax=.3, center=0, annot=True,
square=True, linewidths=5, cbar_kws={"shrink": .5})

colum_name = []
unique_value = []

# Iterate through the columns
for col in data:
    if data[col].dtype == 'object':
        # If 2 or fewer unique categories
        colum_name.append(str(col))
        unique_value.append(data[col].nunique())

table = pd.DataFrame()
table['Col_name'] = colum_name
table['Value'] = unique_value

table = table.sort_values('Value', ascending = False)
table

# Dropping columns based on above result
data.drop(['incident_location', 'policy_bind_date', 'incident_date', 'auto_model', 'insured_occupation', 'policy_number'], axis=1, inplace=True)

f, ax = plt.subplots(figsize=(20,20))
sns.countplot(x='insured_hobbies', hue='fraud_reported', data= data)


data['insured_hobbies'] = data['insured_hobbies'].apply(lambda x : 'Other' if x!='chess' and x!='cross-fit' else x)

f, ax = plt.subplots(figsize=(20,20))
sns.countplot(x='auto_make', hue='fraud_reported', data=data)

# Get unique values of Insured_Hobbies
data['insured_hobbies'].unique()

# Get dummy/indicator values of the dataframe
data = pd.get_dummies(data)
print('Training Features shape:', data.shape)

f, ax = plt.subplots(figsize=(10,10))
sns.countplot(x='fraud_reported', data=data)

corr = data.corr()
y = data['fraud_reported']
X = data.drop('fraud_reported', axis =1)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)

from sklearn.metrics import f1_score

def lgb_f1_score(y_hat, data):
    y_true = data.get_label()
    y_hat = np.round(y_hat)

    return 'f1', f1_score(y_true, y_hat), True

def run_lgb(X_train, X_test, y_train, y_test, test_df):
    params = {
        "objective" : "binary",
       "n_estimators":1000,
       "reg_alpha" : 0.5,
       "reg_lambda":0.5,
       "n_jobs":-1,
       "colsample_bytree":.8,
       "min_child_weight":8,
       "subsample":0.8715623,
       "min_data_in_leaf":30,
       "nthread":4,
       "metric" : "f1",
       "num_leaves" : 10,
       "learning_rate" : 0.01,
       "verbosity" : -1,
       "seed": 60,
       "max_bin":60,
       'max_depth':3,
       'min_gain_to_split':.0222415,
       'scale_pos_weight':1.4,
        'bagging_fraction':0.8
    }

    lgtrain = lgb.Dataset(X_train, label=y_train)
    lgval = lgb.Dataset(X_test, label=y_test)
    evals_result = {}
    model = lgb.train(params, lgtrain, 10000, 
                      valid_sets=[lgtrain, lgval], 
                      early_stopping_rounds=100, 
                      verbose_eval=100, 
                      evals_result=evals_result,feval=lgb_f1_score)
    
    pred_test_y = model.predict(test_df, num_iteration=model.best_iteration)
    return pred_test_y, model, evals_result

pred_test, model, evals_result = run_lgb(X_train, X_test, y_train, y_test, X_test)
print("LightGBM Training Completed...")

from sklearn.metrics import roc_auc_score

roc_auc_score(y_test,pred_test)

from sklearn import metrics
fpr, tpr, threshold = metrics.roc_curve(y_test, pred_test)
roc_auc = metrics.auc(fpr, tpr)
f, ax = plt.subplots(figsize=(10, 10))
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

print('Plot feature importances...')
ax = lgb.plot_importance(model, max_num_features=10)
plt.show()
